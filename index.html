<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="HackAPrompt">
  <meta name="keywords"
    content="Prompt Hacking, Jailbreak, GPT-3">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HackAPrompt</title>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->


  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF
          </a>
          <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
            PromptCap
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition
            </h1>
            <div class="is-size-5">
              <span class="author-block">
                <a style="color:#008AD7;font-weight:normal;">Sander Schulhoff<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a style="color:#c100d7;font-weight:normal;">Jeremy Pinto<sup>*</sup></a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a style="color:#008AD7;font-weight:normal;">Anaum Khan</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://louisbouchard.ai/" style="color:#d7ac00;font-weight:normal;">Louis-François Bouchard</a><sup>2,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://noviscl.github.io/" style="color:#f68946;font-weight:normal;">Chenglei Si</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a style="color:#f68946;font-weight:normal;">Svetlina Anati**</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a style="color:#9ed700;font-weight:normal;">Valen Tagliabue**</a><sup>5</sup>,
              </span>
              <span class="author-block">
                <a style="color:#00d788;font-weight:normal;">Anson Liu Kost**</a><sup>6</sup>,
              </span>
              <span class="author-block">
                <a style="color:#d7006b;font-weight:normal;">Christopher Carnahan**</a><sup>7</sup>,
              </span>
              <span class="author-block">
                <a style="color:#008AD7;font-weight:normal;">Jordan Boyd-Graber</a><sup>1</sup>,
              </span>
            </div>
  
            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">▶ </b>University of
                Maryland</span>
              <span class="author-block"><b style="color:#c100d7; font-weight:normal">▶ </b>Mila</span>
              <span class="author-block"><b style="color:#d7ac00; font-weight:normal">▶ </b>Towards AI</span>
              <span class="author-block"><b style="color:#f68946; font-weight:normal">▶ </b>Stanford University</span>
              <span class="author-block"><b style="color:#9ed700; font-weight:normal">▶ </b>Technical University of Sofia</span>
              <span class="author-block"><b style="color:#00d788; font-weight:normal">▶ </b>University of Milan</span>
              <span class="author-block"><b style="color:#d7006b; font-weight:normal">▶ </b>NYU</span>
              <span class="author-block">&nbsp;&nbsp;<sup>*</sup>Equal Contribution</span>
              <span class="author-block">&nbsp;&nbsp;<sup>**</sup>Competition Winner</span>
            </div>

            
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="HackAPrompt.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

              <!-- code Link. -->
              <span class="link-block">
                <a href="https://www.aicrowd.com/challenges/hackaprompt-2023" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Competition page</span>
                </a>
              </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>HackAPrompt dataset</span>
                  </a>
                </span>

    
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        In this paper, we explore the vulnerability of Large Language Models (LLMs) to prompt hacking, where models are manipulated to follow malicious instructions. Through a global competition, we gathered over 600,000 adversarial prompts against three major LLMs, confirming their susceptibility to such attacks and categorizing these adversarial prompts into a comprehensive taxonomy.
  
      </b>
  

      <img src="./static/images/injection_example.svg" alt="teaser" style="margin-top: 40px;">


    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models (LLMs) are deployed in interactive contexts with
direct user engagement, such as chatbots and writing assistants.

These deployments are vulnerable to prompt injection and jailbreaking
(collectively, prompt hacking), in which models are manipulated to ignore
their original instructions and follow potentially malicious
ones.

Although widely acknowledged as a significant security threat, there is
a dearth of large-scale resources and quantitative studies on prompt hacking.


To address this lacuna, we launch a global prompt hacking competition, which
allows for free-form human input attacks. We elicit 600K+ adversarial prompts
against three state-of-the-art LLMs.


We describe the dataset, which empirically verifies that current LLMs can
indeed be manipulated via prompt hacking. We also present a comprehensive
taxonomical ontology of the types of adversarial prompts.

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<style>
  .video-container {
    display: flex;
    justify-content: center;
  }
</style>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo video</h2>
        <div class="content has-text-justified video-container">
          <iframe width="647" height="364" src="https://www.youtube.com/embed/aln6E5x5TfM" title="Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs (video demo)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Abstract -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="privacy-auditing">A Taxonomical Ontology of Prompt Hacking techniques</h2>
        <div class="content has-text-justified">
          <p>
            We present a comprehensive Taxonomical Ontology of Prompt Hacking techniques, which categorizes various methods used to manipulate Large Language Models (LLMs) through prompt hacking. This taxonomy ranges from simple instructions and cognitive hacking to more complex techniques like context overflow, obfuscation, and code injection, offering a detailed insight into the diverse strategies used in prompt hacking attacks.
          </p>
          <img src="./static/images/ontology.png" alt="Taxonomical Ontology of Prompt Hacking" style="margin-top: 40px;">
          <p>
            <i>Figure 5: A Taxonomical Ontology of Prompt Hacking techniques. Blank lines are hypernyms (i.e., typos are
an instance of obfuscation), while grey arrows are meronyms (i.e., Special Case attacks usually contain a Simple
Instruction). Purple nodes are not attacks themselves but can be a part of attacks. Red nodes are specific examples.
          </i></p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="privacy-auditing">Introducing the HackAPrompt Dataset</h2>
        <div class="content has-text-justified">
          <p>
            This dataset, comprising over 600,000 prompts, is split into two distinct collections: the Playground Dataset and the Submissions Dataset. The Playground Dataset provides a broad overview of the prompt hacking process through completely anonymous prompts tested on the interface, while the Submissions Dataset offers a more detailed insight with refined prompts submitted to the leaderboard, exhibiting a higher success rate of high-quality injections.

          </p><p>
            The origin of this dataset was driven by the need to quantitatively study prompt injection and jailbreaking, phenomena collectively termed prompt hacking. Funded through a combination of prizes and compute support from various companies, the dataset was created without allegiance to any specific entity, ensuring its neutrality and focus on academic rigor.
          </p><p>
            Importantly, this dataset is not just an aggregation of prompts; it represents a conscientious effort to understand and mitigate risks associated with LLMs. By making this dataset publicly available, we aim to raise awareness about the potential risks and challenges posed by these models. We hope that this will prompt (no pun intended) a more responsible use and development of LLMs in various applications, safeguarding against misuse while harnessing their potential for innovation and progress.          </p>
          </p><p>
            The table below contains success rates and total distribution of prompts for the two datasets.
          </p><table>
              <thead>
                  <tr>
                      <th>&nbsp;</th> <!-- Empty header for the first column -->
                      <th>Total Prompts</th>
                      <th>Successful Prompts</th>
                      <th>Success Rate</th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td>Submissions</td>
                      <td>41,596</td>
                      <td>34,641</td>
                      <td style="color: green;">83.2%</td>
                  </tr>
                  <tr>
                      <td>Playground</td>
                      <td>560,161</td>
                      <td>43,295</td>
                      <td style="color: green;">7.7%</td>
                  </tr>
              </tbody>
          </table>
          
          <p>
            <i>Table 2: With a much higher success rate, the
              Submissions Dataset dataset contains a denser quantity of high quality injections. In contract, Playground
              Dataset is much larger and demonstrates competitor
              exploration of the task.
          </i></p>
          
          <!-- Dataset Link. -->
          <span class="link-block" style="display: block; text-align: center;">
            <a href="https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="far fa-images"></i>
              </span>
              <span>HackAPrompt dataset</span>
            </a>
          </span>

        </div>
      </div>
    </div>
  </div>
</section>


      <section class="section is-light" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>
@misc{schulhoff2023ignore,
title={Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition},
author    = {Sander Schulhoff and Jeremy Pinto and Anaum Khan and Louis-François Bouchard and Chenglei Si and Svetlina Anati and Valen Tagliabue and Anson Liu Kost and Christopher Carnahan and Jordan Boyd-Graber},
year={2023},
}
</code></pre>
        </div>
      </section>



      <footer class="footer">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p>
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>
                <p>
                  This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                    code</a> of this website,
                  we just ask that you link back to this page in the footer.
                  Please remember to remove the analytics code included in the header of the website which
                  you do not want on your website.
                </p>
              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>
